{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynbname\n",
    "import os\n",
    "\n",
    "def get_this_ipynb():\n",
    "    \"\"\"Gets this ipynb absolute path\n",
    "\n",
    "    Returns:\n",
    "        str: path\n",
    "    \"\"\"\n",
    "    try:\n",
    "        nb_path = ipynbname.path()\n",
    "        return str(nb_path)\n",
    "    except:\n",
    "        \n",
    "        return globals()['__vsc_ipynb_file__']\n",
    "\n",
    "def list_files(directory):\n",
    "    files = []\n",
    "    for item in os.listdir(directory):\n",
    "        item_path = os.path.join(directory, item)\n",
    "        if os.path.isfile(item_path):\n",
    "            files.append(item)\n",
    "    return files\n",
    "\n",
    "def ensure_directory_exists(directory_path):\n",
    "    # Проверяем, существует ли директория [[6]]\n",
    "    if not os.path.isdir(directory_path):\n",
    "        # Создаем директорию (включая родительские каталоги при необходимости)\n",
    "        os.makedirs(directory_path)\n",
    "    return directory_path\n",
    "\n",
    "def append_to_file(file_path, text):\n",
    "    \"\"\"\n",
    "    Дописывает строку в конец файла. Если файл не существует - создает его.\n",
    "    \n",
    "    :param file_path: Путь к файлу\n",
    "    :param text: Добавляемая строка\n",
    "    \"\"\"\n",
    "    with open(file_path, 'a') as file:  # Режим 'a' для добавления в конец [[3]][[8]]\n",
    "        file.write(text + '\\n')  # Добавляем перенос строки [[3]][[9]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ver. 1\n",
    "slow AF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, urlunparse\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from typing import Set, Dict, Any, Optional\n",
    "from requests_cache import CachedSession\n",
    "from collections import deque\n",
    "from colorsys import hsv_to_rgb\n",
    "from tqdm import tqdm\n",
    "from multiprocessing import Pool\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import threading\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from queue import Queue, Empty\n",
    "\n",
    "# Настройка логгирования\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"website_graph.log\", encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def html_title(html):\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    container = soup.new_tag('div')\n",
    "    container.append(soup)\n",
    "    return container\n",
    "\n",
    "\n",
    "class WebsiteGraph:\n",
    "    def __init__(\n",
    "        self,\n",
    "        start_url: str,\n",
    "        max_depth: int = 2,\n",
    "        domain_filter: Optional[str] = None,\n",
    "        path_regex: Optional[str] = None,\n",
    "        node_size: str = \"degree\",\n",
    "        layout: Optional[Dict[str, Any]] = None,\n",
    "        max_links: int = 10,\n",
    "        expire_after: int = 3000\n",
    "    ):\n",
    "        \"\"\"Класс для построения и визуализации графа веб-сайта.\n",
    "    \n",
    "    Args:\n",
    "        start_url (str): Начальный URL для парсинга.\n",
    "        max_depth (int, optional): Максимальная глубина обхода. Defaults to 2.\n",
    "        domain_filter (str, optional): Домен для фильтрации ссылок. Defaults to None.\n",
    "        path_regex (str, optional): Регулярное выражение для путей. Defaults to None.\n",
    "        node_size (str, optional): Метод расчета размера узлов. Defaults to \"degree\".\n",
    "        layout (Dict[str, Any], optional): Параметры визуализации. Defaults to None.\n",
    "        max_links (int, optional): Максимум ссылок на страницу. Defaults to 10.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Инициализация парсера с параметрами: \"\n",
    "                    f\"start_url={start_url}, max_depth={max_depth}, \"\n",
    "                    f\"domain_filter={domain_filter}, path_regex={path_regex}, \"\n",
    "                    f\"node_size={node_size}, max_links={max_links}\")\n",
    "        \n",
    "        self.graph = nx.DiGraph()\n",
    "        self.start_url = self._normalize_url(start_url)\n",
    "        self.max_depth = max_depth\n",
    "        self.domain = urlparse(self.start_url).netloc\n",
    "        self.domain_filter = domain_filter or self.domain\n",
    "        self.path_regex = re.compile(path_regex) if path_regex else None\n",
    "        self.node_size = node_size\n",
    "        self.layout = layout or {\"physics\": True, \"hierarchical\": False}\n",
    "        self.max_links = max_links\n",
    "        self.expire_after = expire_after\n",
    "        self.visited = set()\n",
    "                # Инициализация кэшированной сессии\n",
    "        self.session = CachedSession(\n",
    "            cache_name=f'cache/{urlparse(self.start_url).netloc}',  # Отдельный кэш для каждого домена [[2]]\n",
    "            expire_after=self.expire_after,\n",
    "            allowable_methods=('GET',)  # Кэшируем только GET-запросы [[7]]\n",
    "        )\n",
    "        # Отключаем ненужные проверки для ускорения\n",
    "        self.session.verify = True  # Используйте с осторожностью! Для HTTPS лучше включить проверку\n",
    "         \n",
    "    def _normalize_url(self, url: str) -> str:\n",
    "        \"\"\"Нормализует URL, удаляя якори и дублирующие слеши.\n",
    "        \n",
    "        Args:\n",
    "            url (str): Исходный URL.\n",
    "            \n",
    "        Returns:\n",
    "            str: Нормализованный URL.\n",
    "        \"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        normalized = urlunparse((\n",
    "            parsed.scheme,\n",
    "            parsed.netloc,\n",
    "            parsed.path.rstrip('/'),  # Удаляем trailing slash\n",
    "            '',  # params\n",
    "            parsed.query,  # Сохраняем query-параметры\n",
    "            ''   # fragment\n",
    "        ))\n",
    "        return normalized\n",
    "\n",
    "    def _get_article_title(self, url: str) -> str:\n",
    "        \"\"\"Упрощенная версия с использованием walrus operator\"\"\"\n",
    "        if (match := re.search(r'/wiki/([^/]+)', url)) and (title := match.group(1)):\n",
    "            return title.replace('_', ' ')\n",
    "        return url.split('//')[-1].split('/')[0]  # Альтернатива для не-wiki URL\n",
    "\n",
    "    def _is_valid_url(self, url: str) -> bool:\n",
    "        \"\"\"Проверяет валидность URL согласно фильтрам.\n",
    "        \n",
    "        Args:\n",
    "            url (str): Проверяемый URL.\n",
    "            \n",
    "        Returns:\n",
    "            bool: True, если URL соответствует условиям.\n",
    "        \"\"\"\n",
    "        parsed = urlparse(url)\n",
    "        is_main_page = parsed.path.lower().endswith('main_page')\n",
    "        valid = (\n",
    "            self.domain_filter in parsed.netloc and\n",
    "            not is_main_page and\n",
    "            (not self.path_regex or self.path_regex.search(parsed.path))\n",
    "        )\n",
    "        if not valid:\n",
    "            logger.debug(f\"URL отклонен: {url} (домен: {parsed.netloc}, путь: {parsed.path})\")\n",
    "        return valid\n",
    "    \n",
    "    def _extract_links(self, url: str):\n",
    "        \"\"\"Извлекает ссылки со страницы с кэшированием.\n",
    "        Args:\n",
    "            url (str): URL страницы для парсинга.\n",
    "        Returns:\n",
    "            Tuple[Set[str], Optional[int]]: Множество валидных ссылок и HTTP-статус.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Парсинг ссылок с: {url}\")\n",
    "        try:\n",
    "            response = self.session.get(url, timeout=5)\n",
    "            response.raise_for_status()  # Вызывает HTTPError для 4xx/5xx\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            links = set()\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                full_url = urljoin(url, link[\"href\"])\n",
    "                normalized_url = self._normalize_url(full_url)\n",
    "                if self._is_valid_url(normalized_url):\n",
    "                    links.add(normalized_url)\n",
    "                    if len(links) >= self.max_links:\n",
    "                        logger.debug(f\"Достигнут лимит ссылок ({self.max_links}) для {url}\")\n",
    "                        break\n",
    "            logger.debug(f\"Найдено {len(links)} валидных ссылок на странице\")\n",
    "            return links, response.status_code  # Возвращаем статус успешного ответа\n",
    "\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            # Обработка HTTP-ошибок (4xx/5xx)\n",
    "            status_code = e.response.status_code if e.response else None\n",
    "            logger.error(f\"HTTP ошибка {status_code} для {url}: {str(e)}\", exc_info=True)\n",
    "            return set(), status_code\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            # Обработка сетевых ошибок (timeout, connection error)\n",
    "            logger.error(f\"Сетевая ошибка для {url}: {str(e)}\", exc_info=True)\n",
    "            return set(), None\n",
    "\n",
    "        except Exception as e:\n",
    "            # Обработка остальных исключений\n",
    "            logger.error(f\"Непредвиденная ошибка для {url}: {str(e)}\", exc_info=True)\n",
    "            return set(), None\n",
    "\n",
    "    def _crawl(self, force_reload: bool = False) -> None:\n",
    "        \"\"\"Рекурсивно обходит сайт, строя граф ссылок.\"\"\"\n",
    "        self.start_crawl_time = time.time()\n",
    "        if not force_reload and self.graph.nodes:\n",
    "            logger.info(\"Используем существующий граф\")\n",
    "            return\n",
    "        logger.info(\"Начинаем обход сайта\")\n",
    "        queue = deque([(self.start_url, 0)])\n",
    "        current_depth = 0  # Начальная глубина\n",
    "        with tqdm(total=self.max_depth, desc=\"Глубина обхода\") as pbar:\n",
    "            while queue:\n",
    "                url, depth = queue.popleft()\n",
    "                logger.debug(f\"Обрабатываем URL: {url} (глубина: {depth}/{self.max_depth})\")\n",
    "                \n",
    "                # Обновляем прогресс-бар при переходе на новый уровень глубины\n",
    "                if depth > current_depth:\n",
    "                    pbar.update(depth - current_depth)\n",
    "                    current_depth = depth\n",
    "                    logger.info(f\"Переход на глубину: {current_depth}\")\n",
    "                    \n",
    "                if depth > self.max_depth or url in self.visited:\n",
    "                    logger.debug(f\"Пропуск URL: {url} (посещено: {url in self.visited}, глубина: {depth})\")\n",
    "                    continue\n",
    "                    \n",
    "                self.visited.add(url)\n",
    "                logger.info(f\"Добавление узла: {url} (глубина: {depth})\")\n",
    "                \n",
    "                links, status_code = self._extract_links(url)\n",
    "                if status_code:  # Если статус получен (успешный запрос)\n",
    "                    self.graph.add_node(\n",
    "                        url,\n",
    "                        title=url,\n",
    "                        label=self._get_article_title(url),\n",
    "                        size=self._calculate_node_size(url),\n",
    "                        status_code=status_code,  # Сохраняем статус\n",
    "                        depth=depth \n",
    "                    )\n",
    "                else:  # Если произошла ошибка\n",
    "                    self.graph.add_node(\n",
    "                        url,\n",
    "                        title=url,\n",
    "                        label=self._get_article_title(url),\n",
    "                        size=self._calculate_node_size(url),\n",
    "                        status_code=0,  # Или другое значение по умолчанию\n",
    "                        depth=depth \n",
    "                    )\n",
    "                \n",
    "                '''links, status = self._extract_links(url)\n",
    "                self.graph.add_node(\n",
    "                    url,\n",
    "                    title=url,\n",
    "                    label=self._get_article_title(url),\n",
    "                    size=self._calculate_node_size(url)\n",
    "                )\n",
    "                '''\n",
    "                for link in links:\n",
    "                    if url != link:\n",
    "                        logger.debug(f\"Добавление связи: {url} -> {link}\")\n",
    "                        #print(f\"Добавление связи: {url} -> {link}\")\n",
    "                        self.graph.add_edge(url, link)\n",
    "                        queue.extend((link, depth+1) for link in links)\n",
    "        self.end_crawl_time = time.time()\n",
    "\n",
    "    def _calculate_node_size(self, node: str) -> int:\n",
    "        \"\"\"Рассчитывает размер узла на основе входящих связей.\n",
    "        \n",
    "        Args:\n",
    "            node (str): URL узла.\n",
    "            \n",
    "        Returns:\n",
    "            int: Размер узла в пикселях.\n",
    "        \"\"\"\n",
    "        in_degree = dict(self.graph.in_degree()).get(node, 0)\n",
    "        size = 10 + 3 * in_degree if self.node_size == \"degree\" else 10\n",
    "        logger.debug(f\"Размер узла {node} рассчитан как {size} (входящих связей: {in_degree})\")\n",
    "        return size\n",
    "      \n",
    "    def _get_node_color(self, node: str) -> str:\n",
    "        \"\"\"Вычисляет цвет узла от светло-зеленого до рыжего\n",
    "        Args:\n",
    "            node (str): URL узла\n",
    "        Returns:\n",
    "            str: HEX-код цвета\n",
    "        \"\"\"\n",
    "        if self.max_degree == self.min_degree:\n",
    "            return \"#D4EDD4\"  # Светло-зеленый для графа без связей [[2]]\n",
    "        \n",
    "        t = (self.graph.in_degree(node) - self.min_degree) / (self.max_degree - self.min_degree)\n",
    "        \n",
    "        # Интерполяция от зеленого (h=0.33) к рыжему (h=0.08) [[6]]\n",
    "        h = 0.33 - 0.25 * t  # 0.33 (зеленый) → 0.08 (рыжий)\n",
    "        s = 0.8 + 0.2 * t    # Увеличение насыщенности для темных оттенков\n",
    "        v = 0.9 + 0.1 * t    # Увеличение яркости для светлых участков\n",
    "        \n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        return '#{:02x}{:02x}{:02x}'.format(\n",
    "            int(r*255), int(g*255), int(b*255)\n",
    "        )\n",
    "\n",
    "    def visualize(self, rebuild: bool = False, force_reload: bool = False) -> None:\n",
    "        \"\"\"Создает HTML-визуализацию графа с использованием pyvis.\"\"\"\n",
    "        if rebuild or force_reload or not self.graph.nodes:\n",
    "            self._crawl(force_reload=force_reload)\n",
    "        logger.info(\"Запуск визуализации графа\")\n",
    "        \n",
    "        \n",
    "        if not self.graph.nodes:\n",
    "            logger.warning(\"Граф пуст - визуализация невозможна\")\n",
    "            return\n",
    "        \n",
    "        # Расчет границ градиента\n",
    "        self.degree_map = dict(self.graph.in_degree())\n",
    "        self.min_degree = min(self.degree_map.values()) if self.degree_map else 1\n",
    "        self.max_degree = max(self.degree_map.values()) if self.degree_map else 1\n",
    "        logger.debug(f\"Градиент границы: min={self.min_degree}, max={self.max_degree}\")\n",
    "\n",
    "        net = Network(\n",
    "            notebook=False,\n",
    "            directed=True,\n",
    "            height=\"800px\",\n",
    "            width=\"100%\",\n",
    "            cdn_resources=\"remote\"\n",
    "        )\n",
    "        \n",
    "        # Настройки подсветки\n",
    "        options = {\n",
    "            \"edges\": {\n",
    "                \"color\": {\n",
    "                    \"color\": \"#2B7CE9\",  # Стандартный цвет\n",
    "                    \"highlight\": \"#FF0000\",  # Цвет подсветки\n",
    "                    \"hover\": \"#FF0000\"       # Цвет при наведении\n",
    "                },\n",
    "                \"selectionWidth\": 3,  # Толщина выделенных рёбер\n",
    "                \"smooth\": False\n",
    "            },\n",
    "            \"interaction\": {\n",
    "                \"hoverConnectedEdges\": True,\n",
    "                \"selectConnectedEdges\": True,  # Автоматический выбор связанных рёбер\n",
    "                \"multiselect\": True,\n",
    "                \"navigationButtons\": True,\n",
    "                \"keyboard\":True,\n",
    "                \"hover\": True,\n",
    "                \"click\": True,\n",
    "            },\n",
    "            \"physics\": {\n",
    "                \"enabled\": True,\n",
    "                \"forceAtlas2Based\": {\n",
    "                    \"gravitationalConstant\": -200,  # Увеличьте отталкивание (от -50 до -500)\n",
    "                    \"springLength\": 500,           # Длина связей между узлами (от 100 до 500)\n",
    "                    \"springConstant\": 0.001,        # Жесткость связей (от 0.001 до 0.1)\n",
    "                    \"damping\": 0.3,                # Затухание движения (0-1)\n",
    "                    \"avoidOverlap\": 1              # Избегать пересечений (0-1)\n",
    "                },\n",
    "                \"stabilization\": {\n",
    "                    \"iterations\": 500,             # Итераций для стабилизации\n",
    "                    \"updateInterval\": 50\n",
    "                }\n",
    "            },\n",
    "            \"nodes\": {\n",
    "                \"allow_html\": True,  # Включаем поддержку HTML\n",
    "                \"shape\": \"box\",  # Обязательно для кликабельности [[8]]\n",
    "                \"font\": {\"size\": 10},\n",
    "                \"color\": {\n",
    "                    \"border\": \"#2B7CE9\",\n",
    "                    \"background\": \"#97C2FC\",\n",
    "                    \"highlight\": {\n",
    "                        \"border\": \"#FF0000\",  # Цвет границы узла при выделении\n",
    "                        \"background\": \"#FFFF00\"\n",
    "                    }\n",
    "                },\n",
    "                \"chosen\": True,\n",
    "                \"style\": \"cursor: pointer;\",\n",
    "                \"shapeProperties\": {\n",
    "                    \"allowHtml\": True  # Правильный параметр вместо allow_html [[9]]\n",
    "                    }\n",
    "            },\n",
    "        \n",
    "            \"configure\": {\n",
    "                \"enabled\": False,\n",
    "                \"filter\": \"nodes,edges\",\n",
    "                \"showButton\": False\n",
    "            },\n",
    "            \"version\": \"9.1.2\" \n",
    "            }\n",
    "\n",
    "        net.set_options(json.dumps(options))\n",
    "        for node in self.graph.nodes:\n",
    "            \n",
    "            # Формирование HTML-подсказки\n",
    "            status_code = self.graph.nodes[node].get('status_code', 0)\n",
    "            in_degree = self.graph.in_degree(node)\n",
    "            status_color = \"#e6ffe6\"  # Зеленый фон по умолчанию\n",
    "            \n",
    "            tooltip = (\n",
    "            f\"<div style='padding: 8px; background: {status_color}'>\"\n",
    "            f\"<b>URL:</b> {node}<br>\"\n",
    "            f\"<b>Status:</b> {status_code}<br>\"\n",
    "            f\"<b>In-Degree:</b> {in_degree}<br>\"\n",
    "            f\"</div>\"\n",
    "            )\n",
    "            \n",
    "            \n",
    "            title = self._get_article_title(node)\n",
    "            if status_code and 400 <= int(status_code) < 600:\n",
    "                color = \"#ffcccc\"  # Красный фон для ошибок\n",
    "            else:\n",
    "                color = self._get_node_color(node)\n",
    "            color = self._get_node_color(node)  # <- Новое вычисление цвета\n",
    "            logger.debug(f\"Добавление узла в визуализацию: {node} (заголовок: {title})\")\n",
    "            logger.debug(f\"Цвет узла {node}: {color} (степень: {self.graph.in_degree(node)})\")\n",
    "            \n",
    "            # В цикле добавления узлов\n",
    "            full_url = node if node.startswith((\"http://\", \"https://\")) else f\"http://{node}\"\n",
    "            escaped_url = full_url.replace(\"'\", \"\\\\'\")  # Экранируем одинарные кавычки [[4]]\n",
    "\n",
    "            \n",
    "            net.add_node(\n",
    "                node,\n",
    "                label=title,\n",
    "                title=tooltip,\n",
    "                size=self._calculate_node_size(node),\n",
    "                color=color,\n",
    "                url=full_url,\n",
    "                allow_html=True,\n",
    "                # Добавляем обработчик клика через JavaScript\n",
    "                onclick=f\"window.open('{escaped_url}', '_blank');\",\n",
    "                shapeProperties={\n",
    "                \"allowHtml\": True  # Корректное название опции [[1]]\n",
    "            },\n",
    "            )\n",
    "\n",
    "        for edge in self.graph.edges:\n",
    "            logger.debug(f\"Добавление связи в визуализацию: {edge[0]} -> {edge[1]}\")\n",
    "            net.add_edge(edge[0], edge[1])\n",
    "\n",
    "        ipynb_dir =  '\\\\'.join(get_this_ipynb().split('\\\\')[:-1])\n",
    "        directory = ensure_directory_exists(ipynb_dir + '\\\\graphs')\n",
    "        try:\n",
    "            graph_num = [int(i.split('.')[0].replace('graph','')) for i in list_files(directory)][-1]+1\n",
    "        except:\n",
    "            graph_num = 0\n",
    "        logger.info(\"Сохранение графа в HTML-файл\")\n",
    "        file = f\"{directory}\\\\graph{graph_num}.html\"\n",
    "        \n",
    "        text = f'{file} | max_depth: {self.max_depth} | max_links: {self.max_links} | crawl time: {self.end_crawl_time - self.start_crawl_time}'\n",
    "        \n",
    "        net.write_html(file, open_browser=True)\n",
    "        append_to_file(ipynb_dir+f'\\\\{self.__name__().split('.')[-1]}.txt',text)\n",
    "        \n",
    "        \n",
    "        logger.info(f\"Graph saved as {file} and opened in browser\")\n",
    "        \n",
    "        \n",
    "        print(text)\n",
    "        \n",
    "        \n",
    "        # Нужно закрыть сессию)\n",
    "        self.session.close()\n",
    "    \n",
    "    def __name__(self):\n",
    "        return 'WebsiteGraph'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Пример использования\\nlogging.disable(logging.CRITICAL)\\nfor i in [1,2,3]:\\n    logger.info(\"Запуск программы\")\\n\\n    graph = WebsiteGraph(\\n        start_url=\"https://en.wikipedia.org/wiki/Data_science\",\\n        max_depth=i,\\n        max_links=10,\\n        path_regex=r\"^/wiki/[A-Za-z_]+$\",\\n        layout={\"physics\": True}\\n    )\\n\\n    graph._crawl()\\n    graph.visualize()'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''# Пример использования\n",
    "logging.disable(logging.CRITICAL)\n",
    "for i in [1,2,3]:\n",
    "    logger.info(\"Запуск программы\")\n",
    "\n",
    "    graph = WebsiteGraph(\n",
    "        start_url=\"https://en.wikipedia.org/wiki/Data_science\",\n",
    "        max_depth=i,\n",
    "        max_links=10,\n",
    "        path_regex=r\"^/wiki/[A-Za-z_]+$\",\n",
    "        layout={\"physics\": True}\n",
    "    )\n",
    "\n",
    "    graph._crawl()\n",
    "    graph.visualize()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results:\n",
    "\n",
    "c:\\Users\\ivant\\Desktop\\proj\\WebsiteGraph\\graphs\\graph0.html | max_depth: 1 | max_links: 10 | crawl time: 1.6000185012817383\n",
    "c:\\Users\\ivant\\Desktop\\proj\\WebsiteGraph\\graphs\\graph2.html | max_depth: 2 | max_links: 10 | crawl time: 12.905709743499756\n",
    "c:\\Users\\ivant\\Desktop\\proj\\WebsiteGraph\\graphs\\graph4.html | max_depth: 3 | max_links: 10 | crawl time: 84.55941534042358\n",
    "c:\\Users\\ivant\\Desktop\\proj\\WebsiteGraph\\graphs\\graph0.html | max_depth: 1 | max_links: 10 | crawl time: 3.058067798614502\n",
    "c:\\Users\\ivant\\Desktop\\proj\\WebsiteGraph\\graphs\\graph1.html | max_depth: 2 | max_links: 10 | crawl time: 20.718218088150024\n",
    "c:\\Users\\ivant\\Desktop\\proj\\WebsiteGraph\\graphs\\graph2.html | max_depth: 3 | max_links: 10 | crawl time: 123.30372500419617"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ver 2\n",
    "fast af\n",
    "multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from urllib.parse import urljoin, urlparse, urlunparse\n",
    "from bs4 import BeautifulSoup\n",
    "import networkx as nx\n",
    "from pyvis.network import Network\n",
    "from requests_cache import CachedSession\n",
    "from colorsys import hsv_to_rgb\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"website_graph.log\", encoding=\"utf-8\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class WebsiteGraphMP: # MP for MultiProcessing\n",
    "    def __init__(self,\n",
    "                 start_url: str,\n",
    "                 max_depth: int = 2,\n",
    "                 max_links: int = 5,\n",
    "                 path_regex: str = None,\n",
    "                 workers: int = 10,\n",
    "                 expire_after: int = 3000,\n",
    "                 node_size: str = \"degree\",\n",
    "                 layout: dict = None,\n",
    "                 ):\n",
    "        self.start_url = self._normalize_url(start_url)\n",
    "        self.max_depth = max_depth\n",
    "        self.max_links = max_links\n",
    "        self.domain = urlparse(self.start_url).netloc\n",
    "        self.path_regex = re.compile(path_regex) if path_regex else None\n",
    "        self.workers = workers\n",
    "        self.expire_after = expire_after\n",
    "        self.node_size = node_size\n",
    "        self.layout = layout or {\"physics\": True, \"hierarchical\": False}\n",
    "        self.graph = nx.DiGraph()\n",
    "        \n",
    "        try:\n",
    "            self.results = self._prev_results()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def detect_communities(self):\n",
    "        from networkx.algorithms import community\n",
    "        return list(community.greedy_modularity_communities(self.graph))\n",
    "     \n",
    "    def _normalize_url(self, url: str) -> str:\n",
    "        parsed = urlparse(url)\n",
    "        normalized = urlunparse((\n",
    "            parsed.scheme,\n",
    "            parsed.netloc,\n",
    "            parsed.path.rstrip('/'),\n",
    "            \"\",\n",
    "            parsed.query,\n",
    "            \"\"\n",
    "        ))\n",
    "        return normalized\n",
    "\n",
    "    def _is_valid_url(self, url: str) -> bool:\n",
    "        parsed = urlparse(url)\n",
    "        # Avoid main page and require same domain\n",
    "        is_main_page = parsed.path.lower().endswith(\"main_page\")\n",
    "        valid = (self.domain in parsed.netloc and not is_main_page and \n",
    "                 (not self.path_regex or self.path_regex.search(parsed.path)))\n",
    "        return valid\n",
    "\n",
    "    def _get_article_title(self, url: str) -> str:\n",
    "        match = re.search(r'/wiki/([^/]+)', url)\n",
    "        if match:\n",
    "            return match.group(1).replace('_', ' ')\n",
    "        return url.split('//')[-1].split('/')[0]\n",
    "\n",
    "    def fetch_page(self, url: str, depth: int, session: CachedSession):\n",
    "        \"\"\"Fetch a page, extract valid links, and return node data with found links.\"\"\"\n",
    "        logger.info(f\"Fetching (depth {depth}): {url}\")\n",
    "        try:\n",
    "            response = session.get(url, timeout=5)\n",
    "            response.raise_for_status()\n",
    "            html = response.text\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            links = set()\n",
    "            for link in soup.find_all(\"a\", href=True):\n",
    "                full_url = urljoin(url, link[\"href\"])\n",
    "                normalized_url = self._normalize_url(full_url)\n",
    "                if self._is_valid_url(normalized_url):\n",
    "                    links.add(normalized_url)\n",
    "                    if len(links) >= self.max_links:\n",
    "                        break\n",
    "            node_data = {\n",
    "                \"title\": self._get_article_title(url),\n",
    "                \"label\": self._get_article_title(url),\n",
    "                \"status_code\": response.status_code,\n",
    "                \"depth\": depth\n",
    "            }\n",
    "            return node_data, links\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            status_code = e.response.status_code if e.response else None\n",
    "            logger.error(f\"HTTP error {status_code} for {url}: {str(e)}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Network error for {url}: {str(e)}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error for {url}: {str(e)}\")\n",
    "        # In case of error, return minimal data with no links.\n",
    "        node_data = {\n",
    "            \"title\": self._get_article_title(url),\n",
    "            \"label\": self._get_article_title(url),\n",
    "            \"status_code\": None,\n",
    "            \"depth\": depth\n",
    "        }\n",
    "        return node_data, set()\n",
    "\n",
    "    def process_url(self, url: str, depth: int, session: CachedSession, visited: set):\n",
    "        \"\"\"Process a URL and update the graph. Always return a set of links for further processing.\"\"\"\n",
    "        if url in visited:\n",
    "            return set()\n",
    "        visited.add(url)\n",
    "        node_data, links = self.fetch_page(url, depth, session)\n",
    "        # Remove self-loops and avoid duplicate edges\n",
    "        valid_links = {link for link in links if link != url}\n",
    "        self.graph.add_node(url, **node_data)\n",
    "        for link in valid_links:\n",
    "            # Avoid duplicate edge creation\n",
    "            if not self.graph.has_edge(url, link):\n",
    "                self.graph.add_edge(url, link)\n",
    "        # Only return links for further processing if within max_depth.\n",
    "        return valid_links if depth < self.max_depth else set()\n",
    "\n",
    "    def crawl(self):\n",
    "        \"\"\"Crawl the website using a ThreadPoolExecutor.\"\"\"\n",
    "        try:\n",
    "            print(f'Predicted time for crawling: {self._predict_time(self.max_depth,self.max_links,self.workers)}')\n",
    "        except:\n",
    "            pass # Because it only works when _prev_results works\n",
    "        \n",
    "        \n",
    "        self.start_crawl_time = time.time()\n",
    "        logger.info(\"Starting crawl\")\n",
    "        session = CachedSession(\n",
    "            cache_name=f\"cache/{self.domain}\",\n",
    "            expire_after=self.expire_after,\n",
    "            allowable_methods=(\"GET\",)\n",
    "        )\n",
    "        session.verify = True\n",
    "        \n",
    "        visited = set()\n",
    "        frontier = [(self.start_url, 0)]\n",
    "        with ThreadPoolExecutor(max_workers=self.workers) as executor:\n",
    "            while frontier:\n",
    "                futures = {}\n",
    "                for url, depth in frontier:\n",
    "                    if url not in visited:\n",
    "                        future = executor.submit(self.process_url, url, depth, session, visited)\n",
    "                        futures[future] = depth\n",
    "                frontier = []\n",
    "                for future in futures:\n",
    "                    links = future.result() or set()\n",
    "                    current_depth = futures[future]\n",
    "                    if current_depth < self.max_depth:\n",
    "                        for link in links:\n",
    "                            if link not in visited:\n",
    "                                frontier.append((link, current_depth + 1))\n",
    "        self.end_crawl_time = time.time()                    \n",
    "        # Нужно закрыть сессию)\n",
    "        session.close()\n",
    "        logger.info(\"Crawl complete\")\n",
    "        \n",
    "    \n",
    "    def _calculate_node_size(self, node: str) -> int:\n",
    "        in_degree = self.graph.in_degree(node)\n",
    "        size = 10 + 3 * in_degree if self.node_size == \"degree\" else 10\n",
    "        return size\n",
    "\n",
    "    def _get_node_color(self, node: str) -> str:\n",
    "        \n",
    "        if self.max_degree == self.min_degree:\n",
    "            return \"#D4EDD4\"\n",
    "        t = (self.graph.in_degree(node) - self.min_degree) / (self.max_degree - self.min_degree)\n",
    "        h = 0.33 - 0.25 * t\n",
    "        s = 0.8 + 0.2 * t\n",
    "        v = 0.9 + 0.1 * t\n",
    "        r, g, b = hsv_to_rgb(h, s, v)\n",
    "        return '#{:02x}{:02x}{:02x}'.format(int(r * 255), int(g * 255), int(b * 255))\n",
    "    \n",
    "    def _prev_results(self):\n",
    "        with open('WebsiteGraphMP.txt') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        names = ['name','max_depth', 'max_links','crawl_time','workers']\n",
    "\n",
    "\n",
    "        results = []\n",
    "        for i in list(map(lambda x: x.split(\"|\"), lines)):\n",
    "            if len(i) == len(names):\n",
    "                results.append([_.split(':')[-1] for _ in i])\n",
    "        results = pd.DataFrame(results, columns=names)\n",
    "        results['name'] = results['name'].transform(lambda x: x.split('\\\\')[-1])\n",
    "        results['max_depth'] = pd.to_numeric(results['max_depth'])\n",
    "        results['max_links'] = pd.to_numeric(results['max_links'])\n",
    "        results['crawl_time'] = pd.to_numeric(results['crawl_time'])\n",
    "        results['workers'] = pd.to_numeric(results['workers'].transform(lambda x: x[:-1]))\n",
    "        results.set_index('name', inplace=True)\n",
    "        results.sort_values('crawl_time', inplace=True)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def _predict_time(self,max_depth,max_links,workers):\n",
    "        gran_mean_times = self.results.groupby(['max_depth','max_links','workers'])[['crawl_time']].agg(['mean'])\n",
    "        noworkers_mean_times = self.results.groupby(['max_depth','max_links'])[['crawl_time']].agg(['mean'])\n",
    "        \n",
    "        if (max_depth,max_links,workers) in gran_mean_times.index:\n",
    "            return gran_mean_times.loc[max_depth,max_links,workers].iloc[0]\n",
    "        \n",
    "        elif (max_depth,max_links) in noworkers_mean_times.index:\n",
    "            return noworkers_mean_times.loc[max_depth,max_links].iloc[0]\n",
    "        \n",
    "        else:\n",
    "            X = self.results[['max_depth','max_links','workers']].values\n",
    "            y = self.results['crawl_time'].values\n",
    "\n",
    "            X = PolynomialFeatures(degree=2).fit_transform(X)\n",
    "            model1 = OLS(y,X)\n",
    "            model1 = model1.fit()\n",
    "\n",
    "\n",
    "            mean_times = self.results[self.results.max_links==10].groupby('max_depth')[['crawl_time']].agg(['mean'])\n",
    "            x = PolynomialFeatures(degree=2).fit_transform(mean_times.index.values.reshape(-1,1))\n",
    "\n",
    "            model2 = OLS(mean_times.values,x)\n",
    "            model2 = model2.fit()\n",
    "\n",
    "            return (model1.predict(PolynomialFeatures(degree=2).fit_transform([[max_depth,max_links,workers]]))[0] + model2.predict(PolynomialFeatures(degree=2).fit_transform([[max_depth]]))[0])/2\n",
    "        \n",
    "    def visualize(self, rebuild: bool = False, force_reload: bool = False, force_file_name : str = ''):\n",
    "        if rebuild or force_reload or not self.graph.nodes:\n",
    "            self.crawl()\n",
    "        degrees = [self.graph.in_degree(n) for n in self.graph.nodes()]\n",
    "        self.min_degree = min(degrees) if degrees else 0\n",
    "        self.max_degree = max(degrees) if degrees else 1\n",
    "        logger.info(\"Starting visualization\")\n",
    "\n",
    "        net = Network(\n",
    "            notebook=False, \n",
    "            directed=True, \n",
    "            height=\"800px\", \n",
    "            width=\"100%\", \n",
    "            cdn_resources=\"remote\",\n",
    "            bgcolor = '#000000',\n",
    "            font_color = '#ffffff'\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Example custom physics to help prevent node overlap\n",
    "        net.repulsion(\n",
    "            node_distance=500,\n",
    "            central_gravity=0.2,\n",
    "            spring_length=200,\n",
    "            spring_strength=0.05,\n",
    "            damping=0.09\n",
    "        )\n",
    "        net.font_color\n",
    "        #net.set_options(json.dumps(options))\n",
    "        for node in self.graph.nodes:\n",
    "            data = self.graph.nodes[node]\n",
    "            status_code = data.get(\"status_code\", 0)\n",
    "            in_degree = self.graph.in_degree(node)\n",
    "            tooltip = (\n",
    "                f\"<div style='padding: 8px; background: #e6ffe6'>\"\n",
    "                f\"<b>URL:</b> {node}<br>\"\n",
    "                f\"<b>Status:</b> {status_code}<br>\"\n",
    "                f\"<b>In-Degree:</b> {in_degree}<br>\"\n",
    "                f\"</div>\"\n",
    "            )\n",
    "            title = data.get(\"label\", node)\n",
    "            color = \"#ffcccc\" if status_code and 400 <= int(status_code) < 600 else self._get_node_color(node)\n",
    "            full_url = node if node.startswith((\"http://\", \"https://\")) else f\"http://{node}\"\n",
    "            escaped_url = full_url.replace(\"'\", \"\\\\'\")\n",
    "            net.add_node(\n",
    "                node,\n",
    "                label=self._get_article_title(title),\n",
    "                title=tooltip,\n",
    "                size=self._calculate_node_size(node),\n",
    "                font={\"size\": self._calculate_node_size(node)},\n",
    "                color=color,\n",
    "                url=full_url,\n",
    "                onclick=f\"window.open('{escaped_url}', '_blank');\",\n",
    "                shapeProperties={\"allowHtml\": True}\n",
    "            )\n",
    "\n",
    "        # Avoid duplicate edges by checking existing net.edges (list of dicts)\n",
    "        for edge in self.graph.edges:\n",
    "            src, dst = edge\n",
    "            if src != dst:\n",
    "                # Check if there's an existing edge from src to dst\n",
    "                if not any(e[\"from\"] == src and e[\"to\"] == dst for e in net.edges):\n",
    "                    net.add_edge(\n",
    "                        src,\n",
    "                        dst,\n",
    "                        color={\n",
    "                            \"color\": \"#2B7CE9\",\n",
    "                            \"highlight\": \"#FF0000\",\n",
    "                            \"hover\": \"#FF0000\"\n",
    "                            },\n",
    "                        selectionWidth=3,\n",
    "                        smooth=False\n",
    "                        )\n",
    "                    \n",
    "        ipynb_dir =  '\\\\'.join(get_this_ipynb().split('\\\\')[:-1])\n",
    "        directory = ensure_directory_exists(ipynb_dir + '\\\\graphs')\n",
    "        \n",
    "        if not len(force_file_name):\n",
    "            \n",
    "            try:\n",
    "                graph_num = max([int(i.split('.')[0].replace('graph','')) for i in list_files(directory) if 'graph' in i.split('.')[0]])+1\n",
    "            except:\n",
    "                graph_num = 0\n",
    "            \n",
    "            file = f\"{directory}\\\\graph{graph_num}.html\"\n",
    "        else:\n",
    "            file = f\"{directory}\\\\{force_file_name}.html\"\n",
    "        logger.info(\"Сохранение графа в HTML-файл\")\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            text = f'{file} | max_depth: {self.max_depth} | max_links: {self.max_links} | crawl time: {self.end_crawl_time - self.start_crawl_time} | workers: {self.workers}'\n",
    "        except:\n",
    "            text = f'{file} | max_depth: {self.max_depth} | max_links: {self.max_links} | workers: {self.workers}'\n",
    "        net.write_html(file, open_browser=True)\n",
    "        append_to_file(ipynb_dir+f'\\\\{self.__name__().split('.')[-1]}.txt',text)\n",
    "        \n",
    "        \n",
    "        logger.info(f\"Graph saved as {file} and opened in browser\")\n",
    "        \n",
    "        \n",
    "        print(text)\n",
    "    \n",
    "        # Magic Methods\n",
    "    def __str__(self):\n",
    "        return (f\"WebsiteGraphMP(start_url='{self.start_url}', nodes={self.graph.number_of_nodes()}, \"\n",
    "                f\"edges={self.graph.number_of_edges()})\")\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (f\"WebsiteGraphMP(start_url='{self.start_url}', max_depth={self.max_depth}, \"\n",
    "                f\"max_links={self.max_links}, workers={self.workers})\")\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if not isinstance(other, WebsiteGraphMP):\n",
    "            return NotImplemented\n",
    "        # Compare start_url and basic graph structure (nodes and edges)\n",
    "        return (self.start_url == other.start_url and\n",
    "                nx.is_isomorphic(self.graph, other.graph))\n",
    "\n",
    "    def __ne__(self, other):\n",
    "        return not self.__eq__(other)\n",
    "    \n",
    "    def __add__(self, other):\n",
    "            if not isinstance(other, WebsiteGraphMP):\n",
    "                return NotImplemented\n",
    "\n",
    "            # Use NetworkX's compose to get the union of the two graphs.\n",
    "            # (Nodes present in both graphs will be merged; edge sets are united.)\n",
    "            new_graph = nx.compose(self.graph, other.graph)\n",
    "\n",
    "            # Invent union logic for parameters:\n",
    "            # For example, use the start_url from self, and pick the more permissive (max) values.\n",
    "            new_max_depth = max(self.max_depth, other.max_depth)\n",
    "            new_max_links = max(self.max_links, other.max_links)\n",
    "            new_workers = max(self.workers, other.workers)\n",
    "            new_expire_after = max(self.expire_after, other.expire_after)\n",
    "\n",
    "            # For regex pattern, choose self's pattern if available, otherwise other.\n",
    "            new_path_regex = None\n",
    "            if self.path_regex and other.path_regex:\n",
    "                new_path_regex = self.path_regex.pattern if len(self.path_regex.pattern) >= len(other.path_regex.pattern) else other.path_regex.pattern\n",
    "            elif self.path_regex:\n",
    "                new_path_regex = self.path_regex.pattern\n",
    "            elif other.path_regex:\n",
    "                new_path_regex = other.path_regex.pattern\n",
    "\n",
    "            # Create a new WebsiteGraph instance with unioned parameters.\n",
    "            new_instance = WebsiteGraphMP(\n",
    "                start_url=self.start_url,  # you can decide which one to use\n",
    "                max_depth=new_max_depth,\n",
    "                max_links=new_max_links,\n",
    "                path_regex=new_path_regex,\n",
    "                workers=new_workers,\n",
    "                expire_after=new_expire_after,\n",
    "                layout=self.layout  # or merge layouts if needed\n",
    "            )\n",
    "            # Set the union graph\n",
    "            new_instance.graph = new_graph\n",
    "\n",
    "            return new_instance\n",
    "    \n",
    "    def __iadd__(self, other):\n",
    "        # In-place union: merge other's graph into self\n",
    "        if not isinstance(other, WebsiteGraphMP):\n",
    "            return NotImplemented\n",
    "        self.graph = nx.compose(self.graph, other.graph)\n",
    "        self.max_depth = max(self.max_depth, other.max_depth)\n",
    "        self.max_links = max(self.max_links, other.max_links)\n",
    "        self.workers = max(self.workers, other.workers)\n",
    "        self.expire_after = max(self.expire_after, other.expire_after)\n",
    "        # For path_regex and layout, you can choose to keep self's parameters.\n",
    "        return self\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        if not isinstance(other, WebsiteGraphMP):\n",
    "            return NotImplemented\n",
    "        # Subtract nodes found in the other graph from self.graph\n",
    "        new_instance = WebsiteGraphMP(\n",
    "            start_url=self.start_url,\n",
    "            max_depth=self.max_depth,\n",
    "            max_links=self.max_links,\n",
    "            path_regex=self.path_regex.pattern if self.path_regex else None,\n",
    "            workers=self.workers,\n",
    "            expire_after=self.expire_after,\n",
    "            layout=self.layout\n",
    "        )\n",
    "        new_instance.graph = self.graph.copy()\n",
    "        for node in other.graph.nodes():\n",
    "            if node in new_instance.graph:\n",
    "                new_instance.graph.remove_node(node)\n",
    "        return new_instance\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Iterate over nodes as (node, attributes) tuples.\n",
    "        return iter(self.graph.nodes(data=True))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.graph.number_of_nodes()\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        # Allow indexing by node ID to get node attributes.\n",
    "        return self.graph.nodes[key]\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        return key in self.graph\n",
    "\n",
    "    def __bool__(self):\n",
    "        return self.graph.number_of_nodes() > 0\n",
    "\n",
    "    def __call__(self):\n",
    "        # Calling the instance triggers a re-crawl.\n",
    "        self.crawl()\n",
    "        return self        \n",
    "    \n",
    "    def __name__(self):\n",
    "        return 'WebsiteGraphMP'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 11:01:09,920 - INFO - Starting program\n",
      "2025-03-25 11:01:09,931 - INFO - Starting crawl\n",
      "2025-03-25 11:01:09,938 - INFO - Fetching (depth 0): https://en.wikipedia.org/wiki/Data_science\n",
      "2025-03-25 11:01:10,020 - INFO - Fetching (depth 1): https://en.wikipedia.org/wiki/Information_science\n",
      "2025-03-25 11:01:10,021 - INFO - Fetching (depth 1): https://en.wikipedia.org/wiki/Scientific_method\n",
      "2025-03-25 11:01:10,027 - INFO - Fetching (depth 1): https://en.wikipedia.org/wiki/Interdisciplinary\n",
      "2025-03-25 11:01:10,070 - INFO - Fetching (depth 1): https://en.wikipedia.org/wiki/Comet_NEOWISE\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted time for crawling: 2.0742762088775635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-25 11:01:10,102 - INFO - Fetching (depth 1): https://en.wikipedia.org/wiki/Scientific_computing\n",
      "2025-03-25 11:01:10,508 - INFO - Fetching (depth 1): https://en.wikipedia.org/wiki/Computer_science\n",
      "2025-03-25 11:01:10,586 - INFO - Fetching (depth 1): https://en.wikipedia.org/wiki/Astronomical_survey\n",
      "2025-03-25 11:01:10,647 - INFO - Fetching (depth 1): https://en.wikipedia.org/wiki/Statistics\n",
      "2025-03-25 11:01:10,874 - INFO - Fetching (depth 1): https://en.wikipedia.org/wiki/Space_telescope\n",
      "2025-03-25 11:01:11,881 - INFO - Crawl complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WebsiteGraphMP(start_url='https://en.wikipedia.org/wiki/Data_science', nodes=86, edges=90)\n",
      "WebsiteGraphMP(start_url='https://en.wikipedia.org/wiki/Data_science', max_depth=1, max_links=10, workers=10)\n",
      "https://en.wikipedia.org/wiki/Data_science {'title': 'Data science', 'label': 'Data science', 'status_code': 200, 'depth': 0}\n",
      "https://en.wikipedia.org/wiki/Information_science {'title': 'Information science', 'label': 'Information science', 'status_code': 200, 'depth': 1}\n",
      "https://en.wikipedia.org/wiki/Scientific_method {'title': 'Scientific method', 'label': 'Scientific method', 'status_code': 200, 'depth': 1}\n",
      "https://en.wikipedia.org/wiki/Interdisciplinary {'title': 'Interdisciplinary', 'label': 'Interdisciplinary', 'status_code': 200, 'depth': 1}\n",
      "https://en.wikipedia.org/wiki/Comet_NEOWISE {'title': 'Comet NEOWISE', 'label': 'Comet NEOWISE', 'status_code': 200, 'depth': 1}\n",
      "https://en.wikipedia.org/wiki/Scientific_computing {'title': 'Scientific computing', 'label': 'Scientific computing', 'status_code': 200, 'depth': 1}\n",
      "https://en.wikipedia.org/wiki/Computer_science {'title': 'Computer science', 'label': 'Computer science', 'status_code': 200, 'depth': 1}\n",
      "https://en.wikipedia.org/wiki/Astronomical_survey {'title': 'Astronomical survey', 'label': 'Astronomical survey', 'status_code': 200, 'depth': 1}\n",
      "https://en.wikipedia.org/wiki/Statistics {'title': 'Statistics', 'label': 'Statistics', 'status_code': 200, 'depth': 1}\n",
      "https://en.wikipedia.org/wiki/Space_telescope {'title': 'Space telescope', 'label': 'Space telescope', 'status_code': 200, 'depth': 1}\n",
      "https://en.wikipedia.org/wiki/Comet_WISE_and_NEOWISE {}\n",
      "https://en.wikipedia.org/wiki/Apsis {}\n",
      "https://en.wikipedia.org/wiki/Orbital_eccentricity {}\n",
      "https://en.wikipedia.org/wiki/List_of_minor_planet_discoverers {}\n",
      "https://en.wikipedia.org/wiki/Astronomical_unit {}\n",
      "https://en.wikipedia.org/wiki/Observation_arc {}\n",
      "https://en.wikipedia.org/wiki/List_of_comets_by_type {}\n",
      "https://en.wikipedia.org/wiki/Osculating_orbit {}\n",
      "https://en.wikipedia.org/wiki/Long_period_comet {}\n",
      "https://en.wikipedia.org/wiki/Research_ethics {}\n",
      "https://en.wikipedia.org/wiki/Argument {}\n",
      "https://en.wikipedia.org/wiki/Transdisciplinarity {}\n",
      "https://en.wikipedia.org/wiki/Interdisciplinarity {}\n",
      "https://en.wikipedia.org/wiki/Academic_writing {}\n",
      "https://en.wikipedia.org/wiki/Research_question {}\n",
      "https://en.wikipedia.org/wiki/Research_proposal {}\n",
      "https://en.wikipedia.org/wiki/Research {}\n",
      "https://en.wikipedia.org/wiki/Research_design {}\n",
      "https://en.wikipedia.org/wiki/Library_science {}\n",
      "https://en.wikipedia.org/wiki/Information_engineering {}\n",
      "https://en.wikipedia.org/wiki/Information_access {}\n",
      "https://en.wikipedia.org/wiki/Information_architecture {}\n",
      "https://en.wikipedia.org/wiki/Outline_of_information_science {}\n",
      "https://en.wikipedia.org/wiki/Information_theory {}\n",
      "https://en.wikipedia.org/wiki/Information_system {}\n",
      "https://en.wikipedia.org/wiki/Algorithm {}\n",
      "https://en.wikipedia.org/wiki/Computational_model {}\n",
      "https://en.wikipedia.org/wiki/Numerical_analysis {}\n",
      "https://en.wikipedia.org/wiki/Sciences {}\n",
      "https://en.wikipedia.org/wiki/Computing {}\n",
      "https://en.wikipedia.org/wiki/Computational_science {}\n",
      "https://en.wikipedia.org/wiki/Mathematical_model {}\n",
      "https://en.wikipedia.org/wiki/Computer_simulation {}\n",
      "https://en.wikipedia.org/wiki/Astrograph {}\n",
      "https://en.wikipedia.org/wiki/Electromagnetic_spectrum {}\n",
      "https://en.wikipedia.org/wiki/Transient_astronomical_event {}\n",
      "https://en.wikipedia.org/wiki/Sky {}\n",
      "https://en.wikipedia.org/wiki/Astronomical_catalog {}\n",
      "https://en.wikipedia.org/wiki/ESO {}\n",
      "https://en.wikipedia.org/wiki/Very_Large_Telescope {}\n",
      "https://en.wikipedia.org/wiki/Celestial_cartography {}\n",
      "https://en.wikipedia.org/wiki/Astrophotography {}\n",
      "https://en.wikipedia.org/wiki/Philosophy_of_science {}\n",
      "https://en.wikipedia.org/wiki/Branches_of_science {}\n",
      "https://en.wikipedia.org/wiki/Epistemology {}\n",
      "https://en.wikipedia.org/wiki/Natural_science {}\n",
      "https://en.wikipedia.org/wiki/Science {}\n",
      "https://en.wikipedia.org/wiki/History_of_science {}\n",
      "https://en.wikipedia.org/wiki/Scientific_literature {}\n",
      "https://en.wikipedia.org/wiki/Formal_science {}\n",
      "https://en.wikipedia.org/wiki/Glossary_of_computer_science {}\n",
      "https://en.wikipedia.org/wiki/Artificial_intelligence {}\n",
      "https://en.wikipedia.org/wiki/Outline_of_computer_science {}\n",
      "https://en.wikipedia.org/wiki/Computational_complexity_theory {}\n",
      "https://en.wikipedia.org/wiki/Programming_language_theory {}\n",
      "https://en.wikipedia.org/wiki/Computer_architecture {}\n",
      "https://en.wikipedia.org/wiki/Computation {}\n",
      "https://en.wikipedia.org/wiki/History_of_computer_science {}\n",
      "https://en.wikipedia.org/wiki/Information {}\n",
      "https://en.wikipedia.org/wiki/Outline_of_statistics {}\n",
      "https://en.wikipedia.org/wiki/Descriptive_statistics {}\n",
      "https://en.wikipedia.org/wiki/Inferential_statistics {}\n",
      "https://en.wikipedia.org/wiki/Statistic {}\n",
      "https://en.wikipedia.org/wiki/Scatter_plot {}\n",
      "https://en.wikipedia.org/wiki/Probability_density_function {}\n",
      "https://en.wikipedia.org/wiki/Iris_flower_data_set {}\n",
      "https://en.wikipedia.org/wiki/Line_chart {}\n",
      "https://en.wikipedia.org/wiki/Normal_distribution {}\n",
      "https://en.wikipedia.org/wiki/Telescope {}\n",
      "https://en.wikipedia.org/wiki/Hubble_Space_Telescope {}\n",
      "https://en.wikipedia.org/wiki/Lyman_Spitzer {}\n",
      "https://en.wikipedia.org/wiki/Twinkling {}\n",
      "https://en.wikipedia.org/wiki/Orbiting_Astronomical_Observatory {}\n",
      "https://en.wikipedia.org/wiki/Light_pollution {}\n",
      "https://en.wikipedia.org/wiki/Observatory {}\n",
      "https://en.wikipedia.org/wiki/Great_Observatories_program {}\n",
      "Total nodes: 86\n",
      "Contains 'https://en.wikipedia.org/wiki/Data_science': True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logger.info(\"Starting program\")\n",
    "graph1 = WebsiteGraphMP(\n",
    "    start_url=\"https://en.wikipedia.org/wiki/Data_science\",\n",
    "    max_depth=1,\n",
    "    max_links=10,\n",
    "    path_regex=r\"^/wiki/[A-Za-z_]+$\",\n",
    "    workers=10,\n",
    "    layout={\"physics\": True}\n",
    ")()\n",
    "# Call instance to crawl\n",
    "\n",
    "print(graph1)  # Uses __str__\n",
    "print(repr(graph1))  # Uses __repr__\n",
    "# Iterate over nodes\n",
    "for node, data in graph1:\n",
    "    print(node, data)\n",
    "# Check length\n",
    "print(\"Total nodes:\", len(graph1))\n",
    "# Check membership\n",
    "print(\"Contains 'https://en.wikipedia.org/wiki/Data_science':\", \"https://en.wikipedia.org/wiki/Data_science\" in graph1)\n",
    "\n",
    "# graph1.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph1 = WebsiteGraphMP(\\n    start_url=\"https://en.wikipedia.org/wiki/Data_science\",\\n    max_depth=4,\\n    max_links=10,\\n    path_regex=r\"^/wiki/[A-Za-z_]+$\",\\n    workers=10,\\n    layout={\"physics\": True}\\n)\\ngraph1()\\n\\ngraph1.visualize(force_file_name = \\'4d.10l.10w\\')'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''graph1 = WebsiteGraphMP(\n",
    "    start_url=\"https://en.wikipedia.org/wiki/Data_science\",\n",
    "    max_depth=4,\n",
    "    max_links=10,\n",
    "    path_regex=r\"^/wiki/[A-Za-z_]+$\",\n",
    "    workers=10,\n",
    "    layout={\"physics\": True}\n",
    ")\n",
    "graph1()\n",
    "\n",
    "graph1.visualize(force_file_name = '4d.10l.10w')'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "results:\n",
    "\n",
    "c:\\Users\\ivant\\Desktop\\proj\\WebsiteGraph\\graphs\\graph3.html | max_depth: 1 | max_links: 10 | crawl time: 1.1849431991577148 | workers: 10\n",
    "c:\\Users\\ivant\\Desktop\\proj\\WebsiteGraph\\graphs\\graph4.html | max_depth: 1 | max_links: 10 | crawl time: 1.4319779872894287 | workers: 20\n",
    "c:\\Users\\ivant\\Desktop\\proj\\WebsiteGraph\\graphs\\graph5.html | max_depth: 2 | max_links: 10 | crawl time: 12.662982702255249 | workers: 10\n",
    "c:\\Users\\ivant\\Desktop\\proj\\WebsiteGraph\\graphs\\graph6.html | max_depth: 2 | max_links: 10 | crawl time: 13.79235053062439 | workers: 20\n",
    "c:\\Users\\ivant\\Desktop\\proj\\WebsiteGraph\\graphs\\graph7.html | max_depth: 3 | max_links: 10 | crawl time: 66.65576791763306 | workers: 10\n",
    "c:\\Users\\ivant\\Desktop\\proj\\WebsiteGraph\\graphs\\graph8.html | max_depth: 3 | max_links: 10 | crawl time: 62.48543572425842 | workers: 20\n",
    "c:\\Users\\ivant\\Desktop\\proj\\WebsiteGraph\\graphs\\graph9.html | max_depth: 4 | max_links: 10 | crawl time: 264.98015880584717 | workers: 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`max_depth=4,\n",
    "max_links=10`\n",
    "<br>\n",
    "С этими параметрами - визуализация проблематична, но строится относительно быстро, можно пользоваться как графом этой структурой можно спокойно"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Union graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'graph1 = WebsiteGraphMP(start_url=\"https://en.wikipedia.org/wiki/Data_science\", max_depth=1, max_links=10, path_regex=r\"^/wiki/[A-Za-z_]+$\")\\ngraph1()\\n\\ngraph2 = WebsiteGraphMP(start_url=\"https://en.wikipedia.org/wiki/Artificial_intelligence\", max_depth=1, max_links=10, path_regex=r\"^/wiki/[A-Za-z_]+$\")\\ngraph2()\\n\\nunion_graph = graph1 + graph2\\nunion_graph.visualize()'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''graph1 = WebsiteGraphMP(start_url=\"https://en.wikipedia.org/wiki/Data_science\", max_depth=1, max_links=10, path_regex=r\"^/wiki/[A-Za-z_]+$\")\n",
    "graph1()\n",
    "\n",
    "graph2 = WebsiteGraphMP(start_url=\"https://en.wikipedia.org/wiki/Artificial_intelligence\", max_depth=1, max_links=10, path_regex=r\"^/wiki/[A-Za-z_]+$\")\n",
    "graph2()\n",
    "\n",
    "union_graph = graph1 + graph2\n",
    "union_graph.visualize()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "crawl_time  mean    286.954203\n",
       "dtype: float64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = graph1._prev_results()\n",
    "gran_mean_times = results.groupby(['max_depth','max_links','workers'])[['crawl_time']].agg(['mean'])\n",
    "gran_mean_times.loc[4,10].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>max_depth</th>\n",
       "      <th>max_links</th>\n",
       "      <th>crawl_time</th>\n",
       "      <th>workers</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>wtf.html</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.053846</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>graph11.html</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.061337</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>graph10.html</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.067775</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>graph10.html</th>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.072039</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>graph12.html</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.108873</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>graph10.html</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.119081</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>graph10.html</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.378627</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>graph3.html</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1.184943</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>graph4.html</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1.431978</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>graph10.html</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3.873815</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>graph5.html</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>12.662983</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>graph6.html</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>13.792351</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>graph8.html</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>62.485436</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>graph7.html</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>66.655768</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>graph9.html</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>264.980159</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4d.10l.20w.html</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>295.078492</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4d.10l.10w.html</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>322.778004</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  max_depth  max_links  crawl_time  workers\n",
       "name                                                       \n",
       "wtf.html                  0          5    0.053846       10\n",
       "graph11.html              0          5    0.061337       10\n",
       "graph10.html              0          5    0.067775       10\n",
       "graph10.html              0         10    0.072039       10\n",
       "graph12.html              0          5    0.108873       10\n",
       "graph10.html              0          5    0.119081       10\n",
       "graph10.html              0          5    0.378627       10\n",
       "graph3.html               1         10    1.184943       10\n",
       "graph4.html               1         10    1.431978       20\n",
       "graph10.html              1         10    3.873815       10\n",
       "graph5.html               2         10   12.662983       10\n",
       "graph6.html               2         10   13.792351       20\n",
       "graph8.html               3         10   62.485436       20\n",
       "graph7.html               3         10   66.655768       10\n",
       "graph9.html               4         10  264.980159       10\n",
       "4d.10l.20w.html           4         10  295.078492       20\n",
       "4d.10l.10w.html           4         10  322.778004       20"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from statsmodels.api import OLS\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X = results[['max_depth','max_links','workers']].values\n",
    "y = results['crawl_time'].values\n",
    "\n",
    "X = PolynomialFeatures(degree=2).fit_transform(X)\n",
    "model = OLS(y,X)\n",
    "model = model.fit()\n",
    "model.summary()\n",
    "\n",
    "model.predict(PolynomialFeatures(degree=2).fit_transform([[10,10,10]]))[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
